---
layout: post
title:  "심화세션 - NLP.week4-1"
date:   2024-10-08 12:00 +09:00
categories: AI
---

# How large language models work, a visual intro to transformers | Chapter 5, Deep Learning

### GPT
- **GPT란 Generative Pre-trained Transformer(생성형 사전 훈련 트랜스포머) 약자**
    - Generative: 텍스트를 생성하는 봇
    - Pre-trained: 모델이 방대한 양의 데이터로부터 학습을 거침, fine-tuning의 여지 남아 있음
    - Transformer: 신경망의 일종


### Transformer
- voice-to-text: 음성을 텍스트로 변환
- text-to-voice: 텍스트를 음성으로 변환
- text-to-image: 텍스트를 기반으로 이미지를 생성
- translation: 텍스트를 다른 언어로 번역
- 텍스트를 청크(작은 조각)로 나누어 다음에 나올 내용을 예측
    - ![alt text](https://github.com/hyeran1216/hyeran1216.github.io/blob/2d575e75da7e2f690fe2d2c86ec712cf6314019f/_posts/images/nlp-12.png?raw=true)
    - 텍스트 청크에 대해 확률 분포 형태를 취함

- 텍스트 생성 과정
    - 긴 텍스트를 생성할 때, 처음에 모델이 짧은 초기 스니펫(시작 구절)을 입력받음
    - 방금 생성한 확률 분포에서 무작위 샘플을 선택해 해당 텍스트에 추가한 다음 새롭게 추가된 텍스트를 포함한 상태로 다시 예측을 수행
    - 입력된 텍스트는 토큰(단어 조각)으로 나뉨
        - ![alt text](https://github.com/hyeran1216/hyeran1216.github.io/blob/2d575e75da7e2f690fe2d2c86ec712cf6314019f/_posts/images/nlp-13.png?raw=true)
        - 이미지나 소리의 경우 토큰은 해당 이미지의 작은 조각이나 사운드의 작은 덩어리가 됨
        - ![alt text](https://github.com/hyeran1216/hyeran1216.github.io/blob/2d575e75da7e2f690fe2d2c86ec712cf6314019f/_posts/images/nlp-14.png?raw=true)


- 임베딩(Embedding)
    - 각 토큰을 고차원 벡터와 연결
    - 임베딩을 점진적으로 조정해 단순히 개별 단어를 인코딩하는 게 아니라 문맥적 의미를 담을 수 있도록 하는 것
    - 벡터를 매우 고차원적인 공간에 좌표로 제공한다면 비슷한 의미를 가진 단어는 해당 공간에서 서로 가까운 벡터에 위치
    - ![alt text](https://github.com/hyeran1216/hyeran1216.github.io/blob/2d575e75da7e2f690fe2d2c86ec712cf6314019f/_posts/images/nlp-15.png?raw=true)


- 어텐션 블록(Attention Block)
    - 이 일련의 벡터는 어텐션 블록이라고 하는 연산을 통과
    - ![alt text](https://github.com/hyeran1216/hyeran1216.github.io/blob/2d575e75da7e2f690fe2d2c86ec712cf6314019f/_posts/images/nlp-16.png?raw=true)
    - 벡터가 서로 대화하고 정보를 주고받으며 값 업데이트 가능  
    - 어텐션 블록은 문맥에서 어떤 단어가 다른 단어의 의미 업데이트와 관련이 있는지 그리고 그 의미를 어떻게 업데이트해야하는지 파악하는 역할을 함
    - ![alt text](https://github.com/hyeran1216/hyeran1216.github.io/blob/2d575e75da7e2f690fe2d2c86ec712cf6314019f/_posts/images/nlp-17.png?raw=true)
    - 이후 이 벡터는 읽는 소스에 따라 다층 퍼셉트론을 거침
        - ![alt text](https://github.com/hyeran1216/hyeran1216.github.io/blob/2d575e75da7e2f690fe2d2c86ec712cf6314019f/_posts/images/nlp-18.png?raw=true)
        - 여기선 모두 동일한 연산을 병렬로 진행 


- 이 과정 반복
- ![alt text](https://github.com/hyeran1216/hyeran1216.github.io/blob/2d575e75da7e2f690fe2d2c86ec712cf6314019f/_posts/images/nlp-19.png?raw=true)
- 이후 시퀀스의 마지막 벡터에서 다음에 나올 단어의 확률 분포를 생성


### Deep Learning
- 딥러닝은 코드를 명시적으로 작성하는 대신 여러 개의 조정 가능한 매개변수(노브, 다이얼)로 유연한 구조를 설정
- 그 다음 주어진 입력에 대해 출력이 어떤 모습일지 예측하기 위해 매개변수값을 조정하며 학습
- 역전파 알고리즘 사용
- 입력 형식이 실수 배열(보통 텐서)로 지정된 상태에서 점진적으로 여러 계층을 통해 변환
- 이를 통해 최종 출력값 생성


### Word embedding
- 단어 벡터는 고차원 공간에서 서로 유사한 의미를 가진 단어들끼리 가까운 위치에 놓임
- 예시: '남자'와 '여자' 단어의 벡터 차이는 '왕'과 '여왕' 단어의 벡터 차이와 비슷
- ![alt text](https://github.com/hyeran1216/hyeran1216.github.io/blob/2d575e75da7e2f690fe2d2c86ec712cf6314019f/_posts/images/nlp-20.png?raw=true)
- 두 단어 벡터의 내적값은 그 벡터들이 얼마나 잘 정렬되어 있는지를 측정
    - 벡터가 비슷한 방향을 가리키면 양수, 수직이면 0, 반대 방향이면 음수가 나옴


- 임베딩 벡터는 다른 단어 벡터들과의 의미 관계에 따라 가중치가 조정되며, 이를 통해 의미가 밀고 당겨짐으로써 변화


### Unembedding
- Unembedding Matrix는 어휘의 각 단어마다 하나의 행을 가지며 각 행은 임베딩 차원과 동일한 수의 요소를 가짐
- 입력 텍스트를 다시 단어로 변환하는 과정

### Softmax
- 일련의 숫자가 확률분포로 분포하려면 0~1 사이 값 가져야하며 모든 값을 더했을 때 1이여야 함
- 그러나 실제 벡터 연산 수행 시 이렇게 결과가 나오지 않음
    - 음수값이 나오거나 값이 1보다 큰 경우 많음
    - 이걸 확률분포 값으로 변한해주는 게 **Softmax**


- Softmax와 온도(Temperature)
- 지수부 분모에 상수 t를 넣음
- 온도(특정 열역학 방정식에서 온도의 역할과 비슷)
    - 모델이 어느 정도 확률로 예측할지를 조정하는 변수
    - ![alt text](https://github.com/hyeran1216/hyeran1216.github.io/blob/2d575e75da7e2f690fe2d2c86ec712cf6314019f/_posts/images/nlp-9.png?raw=true)
    - 온도가 높을수록 
        - 낮은 값에 가중치를 부여해 확률 분포가 균일해짐
        - 낮은 가능성의 단어들도 선택될 확률이 높아짐
        - 그 결과로 다소 이상한 문장이 나올 수 있음


    - 온도가 낮을수록
        - 큰 값이 더 공격적으로 지배
        - 극단적으로 t를 0으로 설정하면 최대값으로 이동
        - 높은 확률을 가진 단어들이 선택되며
        - 예측 가능한 문장이 나올 확률이 높아짐
        - 예시: 온도 0은 항상 예상 가능한 단어만 사용 -> 조금 진부


    - 기술적으로 2 이상의 온도는 불가(너무 의미 없기 때문에 임의로 설정한 제약조건) 


- Softmax에서의 입력값을 **로짓(logit)**, Softmax를 거친 출력값은 **확률(probability)**이라고 부름
